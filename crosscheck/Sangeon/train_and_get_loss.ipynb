{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Autoencoder Training for Cross-check step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "import torch.utils.data as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_hdf(\"/data/t3home000/spark/LHCOlympics_previous/LHC-Olympics/Code/Nsubjettiness_mjj.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98.677270</td>\n",
       "      <td>0.528903</td>\n",
       "      <td>0.788281</td>\n",
       "      <td>0.904471</td>\n",
       "      <td>4.241889</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1285.895950</td>\n",
       "      <td>53.519023</td>\n",
       "      <td>0.668562</td>\n",
       "      <td>0.735745</td>\n",
       "      <td>0.755674</td>\n",
       "      <td>1.895988</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1282.286017</td>\n",
       "      <td>2577.571899</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>584.595432</td>\n",
       "      <td>0.345626</td>\n",
       "      <td>0.463461</td>\n",
       "      <td>0.865982</td>\n",
       "      <td>1.069972</td>\n",
       "      <td>320.0</td>\n",
       "      <td>1334.493332</td>\n",
       "      <td>405.034096</td>\n",
       "      <td>0.264362</td>\n",
       "      <td>0.793461</td>\n",
       "      <td>0.830032</td>\n",
       "      <td>1.377217</td>\n",
       "      <td>348.0</td>\n",
       "      <td>1306.137883</td>\n",
       "      <td>3807.507389</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>159.597526</td>\n",
       "      <td>0.677692</td>\n",
       "      <td>0.690707</td>\n",
       "      <td>0.695322</td>\n",
       "      <td>1.310040</td>\n",
       "      <td>332.0</td>\n",
       "      <td>678.557182</td>\n",
       "      <td>113.768840</td>\n",
       "      <td>0.713481</td>\n",
       "      <td>0.922610</td>\n",
       "      <td>0.782783</td>\n",
       "      <td>1.887494</td>\n",
       "      <td>236.0</td>\n",
       "      <td>1072.462085</td>\n",
       "      <td>1710.965414</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>515.237299</td>\n",
       "      <td>0.091038</td>\n",
       "      <td>0.784454</td>\n",
       "      <td>0.860716</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>248.0</td>\n",
       "      <td>1284.020224</td>\n",
       "      <td>161.648798</td>\n",
       "      <td>0.727507</td>\n",
       "      <td>0.719564</td>\n",
       "      <td>0.870109</td>\n",
       "      <td>1.997360</td>\n",
       "      <td>352.0</td>\n",
       "      <td>1217.031950</td>\n",
       "      <td>2603.379037</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142.420213</td>\n",
       "      <td>0.507714</td>\n",
       "      <td>0.522686</td>\n",
       "      <td>0.904070</td>\n",
       "      <td>1.853319</td>\n",
       "      <td>220.0</td>\n",
       "      <td>1087.658980</td>\n",
       "      <td>105.721163</td>\n",
       "      <td>0.344534</td>\n",
       "      <td>0.614579</td>\n",
       "      <td>0.863765</td>\n",
       "      <td>1.113248</td>\n",
       "      <td>204.0</td>\n",
       "      <td>1205.343324</td>\n",
       "      <td>3294.162200</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4      5            6  \\\n",
       "0   98.677270  0.528903  0.788281  0.904471  4.241889  136.0  1285.895950   \n",
       "1  584.595432  0.345626  0.463461  0.865982  1.069972  320.0  1334.493332   \n",
       "2  159.597526  0.677692  0.690707  0.695322  1.310040  332.0   678.557182   \n",
       "3  515.237299  0.091038  0.784454  0.860716  1.102743  248.0  1284.020224   \n",
       "4  142.420213  0.507714  0.522686  0.904070  1.853319  220.0  1087.658980   \n",
       "\n",
       "            7         8         9        10        11     12           13  \\\n",
       "0   53.519023  0.668562  0.735745  0.755674  1.895988  128.0  1282.286017   \n",
       "1  405.034096  0.264362  0.793461  0.830032  1.377217  348.0  1306.137883   \n",
       "2  113.768840  0.713481  0.922610  0.782783  1.887494  236.0  1072.462085   \n",
       "3  161.648798  0.727507  0.719564  0.870109  1.997360  352.0  1217.031950   \n",
       "4  105.721163  0.344534  0.614579  0.863765  1.113248  204.0  1205.343324   \n",
       "\n",
       "            14   15  \n",
       "0  2577.571899  0.0  \n",
       "1  3807.507389  0.0  \n",
       "2  1710.965414  0.0  \n",
       "3  2603.379037  0.0  \n",
       "4  3294.162200  1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = f.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt[:,0] = (dt[:,0]-np.mean(dt[:,0]))/np.std(dt[:,0])\n",
    "dt[:,1] = (dt[:,1]-np.mean(dt[:,1]))/np.std(dt[:,1])\n",
    "dt[:,2] = (dt[:,2]-np.mean(dt[:,2]))/np.std(dt[:,2])\n",
    "dt[:,3] = (dt[:,3]-np.mean(dt[:,3]))/np.std(dt[:,3])\n",
    "dt[:,4] = (dt[:,4]-np.mean(dt[:,4]))/np.std(dt[:,4])\n",
    "dt[:,5] = (dt[:,5]-np.mean(dt[:,5]))/np.std(dt[:,5])\n",
    "\n",
    "dt[:,7] = (dt[:,7]-np.mean(dt[:,7]))/np.std(dt[:,7])\n",
    "dt[:,8] = (dt[:,8]-np.mean(dt[:,8]))/np.std(dt[:,8])\n",
    "dt[:,9] = (dt[:,9]-np.mean(dt[:,9]))/np.std(dt[:,9])\n",
    "dt[:,10] = (dt[:,10]-np.mean(dt[:,10]))/np.std(dt[:,10])\n",
    "dt[:,11] = (dt[:,11]-np.mean(dt[:,11]))/np.std(dt[:,11])\n",
    "dt[:,12] = (dt[:,12]-np.mean(dt[:,12]))/np.std(dt[:,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = dt[:,15]\n",
    "bkg_idx = np.where(idx==0)[0]\n",
    "signal_idx = np.where(idx==1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second architecture\n",
    "class Encoder_jet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(12, 48)\n",
    "        self.linear2 = nn.Linear(48, 30)\n",
    "        self.linear3 = nn.Linear(30, 20)\n",
    "        self.linear4 = nn.Linear(20, 10)\n",
    "        self.linear5 = nn.Linear(10, 6)\n",
    "        self.linear6 = nn.Linear(6, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.linear1(x))\n",
    "        x = F.leaky_relu(self.linear2(x))\n",
    "        x = F.leaky_relu(self.linear3(x))\n",
    "        x = F.leaky_relu(self.linear4(x))\n",
    "        x = F.leaky_relu(self.linear5(x))\n",
    "        x = F.tanh(self.linear6(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_jet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(3, 6)\n",
    "        self.linear2 = nn.Linear(6, 10)\n",
    "        self.linear3 = nn.Linear(10, 20)\n",
    "        self.linear4 = nn.Linear(20, 30)\n",
    "        self.linear5 = nn.Linear(30, 48)\n",
    "        self.linear6 = nn.Linear(48, 12)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.linear1(x))\n",
    "        x = F.leaky_relu(self.linear2(x))\n",
    "        x = F.leaky_relu(self.linear3(x))\n",
    "        x = F.leaky_relu(self.linear4(x))\n",
    "        x = F.leaky_relu(self.linear5(x))\n",
    "        x = self.linear6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanarFlow(nn.Module):\n",
    "    def __init__(self, D):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "\n",
    "    def forward(self, z, lamda):\n",
    "        '''\n",
    "        z - latents from prev layer\n",
    "        lambda - Flow parameters (b, w, u)\n",
    "        b - scalar\n",
    "        w - vector\n",
    "        u - vector\n",
    "        '''\n",
    "        b = lamda[:, :1]\n",
    "        w, u = lamda[:, 1:].chunk(2, dim=1)\n",
    "\n",
    "        # Forward\n",
    "        # f(z) = z + u tanh(w^T z + b)\n",
    "        transf = F.tanh(\n",
    "            z.unsqueeze(1).bmm(w.unsqueeze(2))[:, 0] + b\n",
    "        )\n",
    "        f_z = z + u * transf\n",
    "\n",
    "        # Inverse\n",
    "        # psi_z = tanh' (w^T z + b) w\n",
    "        psi_z = (1 - transf ** 2) * w\n",
    "        log_abs_det_jacobian = torch.log(\n",
    "            (1 + psi_z.unsqueeze(1).bmm(u.unsqueeze(2))).abs()\n",
    "        )\n",
    "\n",
    "        return f_z, log_abs_det_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizingFlow(nn.Module):\n",
    "    def __init__(self, K, D):\n",
    "        super().__init__()\n",
    "        self.flows = nn.ModuleList([PlanarFlow(D) for i in range(K)])\n",
    "\n",
    "    def forward(self, z_k, flow_params):\n",
    "        # ladj -> log abs det jacobian\n",
    "        sum_ladj = 0\n",
    "        for i, flow in enumerate(self.flows):\n",
    "            z_k, ladj_k = flow(z_k, flow_params[i])\n",
    "            sum_ladj += ladj_k\n",
    "\n",
    "        return z_k, sum_ladj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_NF(nn.Module):\n",
    "    def __init__(self, K, D):\n",
    "        super().__init__()\n",
    "        self.dim = D\n",
    "        self.K = K\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(22, 96),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(96, 48),\n",
    "            nn.LeakyReLU(True),            \n",
    "            nn.Linear(48, D * 2 + K * (D * 2 + 1))\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(D, 48),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(48, 96),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(96, 22),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.flows = NormalizingFlow(K, D)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run Encoder and get NF params\n",
    "        enc = self.encoder(x)\n",
    "        mu = enc[:, :self.dim]\n",
    "        log_var = enc[:, self.dim: self.dim * 2]\n",
    "        flow_params = enc[:, 2 * self.dim:].chunk(self.K, dim=1)\n",
    "\n",
    "        # Re-parametrize\n",
    "        sigma = (log_var * .5).exp()\n",
    "        z = mu + sigma * torch.randn_like(sigma)\n",
    "        kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "        # Construct more expressive posterior with NF\n",
    "        z_k, sum_ladj = self.flows(z, flow_params)\n",
    "        kl_div = kl_div / x.size(0) - sum_ladj.mean()  # mean over batch\n",
    "\n",
    "        # Run Decoder\n",
    "        x_prime = self.decoder(z_k)\n",
    "        return x_prime, kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_idx(dt, train_idx, bb_idx, num_sig):\n",
    "    data_train_temp = torch.tensor(dt[train_idx,:])\n",
    "    data_train_temp_1 = data_train_temp.t()[0:6].t()\n",
    "    data_train_temp_2 = data_train_temp.t()[7:13].t()\n",
    "    data_train = torch.cat((data_train_temp_1,data_train_temp_2),dim=1)\n",
    "    bs = 100\n",
    "    my_dataset = utils.TensorDataset(data_train,data_train) \n",
    "    my_dataloader = utils.DataLoader(my_dataset, batch_size=bs, shuffle=True)\n",
    "    sig_model_enc = Encoder_jet().cuda()\n",
    "    sig_model_dec = Decoder_jet().cuda()\n",
    "    lrs = 1e-4\n",
    "    sig_optimizer_enc = optim.Adam(sig_model_enc.parameters(), lr = lrs)\n",
    "    sig_optimizer_dec = optim.Adam(sig_model_dec.parameters(), lr = lrs)\n",
    "    sig_loss = nn.MSELoss()\n",
    "    epochs = 80\n",
    "    patience_counter = 0\n",
    "    min_loss = 9999\n",
    "    for epoch in range(epochs):\n",
    "        sig_model_enc.train()\n",
    "        sig_model_dec.train()\n",
    "        for i, dtt in enumerate(my_dataloader):\n",
    "            sig_optimizer_enc.zero_grad()\n",
    "            sig_optimizer_dec.zero_grad()\n",
    "            \n",
    "            inp = dtt[0].float().cuda()\n",
    "            output = dtt[1].float().cuda()\n",
    "            ls = sig_loss(sig_model_dec(sig_model_enc(inp)),output)\n",
    "            ls.backward()\n",
    "            sig_optimizer_enc.step()\n",
    "            sig_optimizer_dec.step()\n",
    "        \n",
    "        print(\"Epoch: \" + str(epoch), \"Loss Training: \" + str(ls.data.cpu().numpy()))\n",
    "        if ls.data.cpu().numpy() < min_loss:\n",
    "            min_loss = ls.data.cpu().numpy()\n",
    "            patience_counter = 0\n",
    "            torch.save(sig_model_enc.state_dict(), f\"sigae_enc_{num_sig}.h5\")\n",
    "            torch.save(sig_model_dec.state_dict(), f\"sigae_dec_{num_sig}.h5\")\n",
    "            print('patience counter: ',patience_counter)\n",
    "    \n",
    "            \n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print('patience counter: ',patience_counter)\n",
    "            if patience_counter >= 10:\n",
    "                print('maximum patience reached')\n",
    "                break\n",
    "            \n",
    "    data_bb_temp = torch.tensor(dt[bb_idx,:])\n",
    "    data_bb_temp_1 = data_bb_temp.t()[0:6].t()\n",
    "    data_bb_temp_2 = data_bb_temp.t()[7:13].t()\n",
    "    data_bb = torch.cat((data_bb_temp_1,data_bb_temp_2),dim=1)\n",
    "    signalAE_loss = torch.mean((sig_model_dec(sig_model_enc(data_bb.float().cuda()))-data_bb.float().cuda())**2,dim=1).data.cpu().numpy()\n",
    "    np.save(f'sigaeloss_numsig_{num_sig}_bb',signalAE_loss)\n",
    "    \n",
    "    signalAE_trainloss = torch.mean((sig_model_dec(sig_model_enc(data_train.float().cuda()))-data_train.float().cuda())**2,dim=1).data.cpu().numpy()\n",
    "    np.save(f'sigaeloss_numsig_{num_sig}_trainloss',signalAE_trainloss)\n",
    "    \n",
    "    f = pd.read_hdf(\"/data/t3home000/spark/LHCOlympics/data/MassRatio_pureBkg.h5\")\n",
    "    dt = f.values\n",
    "    dt[:,1] = (dt[:,1]-np.mean(dt[:,1]))/np.std(dt[:,1])\n",
    "    dt[:,2] = (dt[:,2]-np.mean(dt[:,2]))/np.std(dt[:,2])\n",
    "    dt[:,3] = (dt[:,3]-np.mean(dt[:,3]))/np.std(dt[:,3])\n",
    "    dt[:,4] = (dt[:,4]-np.mean(dt[:,4]))/np.std(dt[:,4])\n",
    "    dt[:,5] = (dt[:,5]-np.mean(dt[:,5]))/np.std(dt[:,5])\n",
    "    dt[:,6] = (dt[:,6]-np.mean(dt[:,6]))/np.std(dt[:,6])\n",
    "\n",
    "    dt[:,14] = (dt[:,14]-np.mean(dt[:,14]))/np.std(dt[:,14])\n",
    "    dt[:,15] = (dt[:,15]-np.mean(dt[:,15]))/np.std(dt[:,15])\n",
    "    dt[:,16] = (dt[:,16]-np.mean(dt[:,16]))/np.std(dt[:,16])\n",
    "    dt[:,17] = (dt[:,17]-np.mean(dt[:,17]))/np.std(dt[:,17])\n",
    "    dt[:,18] = (dt[:,18]-np.mean(dt[:,18]))/np.std(dt[:,18])\n",
    "    dt[:,19] = (dt[:,19]-np.mean(dt[:,19]))/np.std(dt[:,19])\n",
    "    data = torch.tensor(dt)\n",
    "    data_train_x_1 = data.t()[1:7].t()\n",
    "    data_train_x_2 = data.t()[14:20].t()\n",
    "    data_bkg = torch.cat((data_train_x_1,data_train_x_2),dim=1)\n",
    "    signalAE_loss_purebkg = torch.mean((sig_model_dec(sig_model_enc(data_bkg.float().cuda()))-data_bkg.float().cuda())**2,dim=1).data.cpu().numpy()\n",
    "    np.save(f'sigaeloss_numsig_{num_sig}_bkg',signalAE_loss_purebkg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "N_EPOCHS = 80\n",
    "PRINT_INTERVAL = 2000\n",
    "NUM_WORKERS = 4\n",
    "LR = 1e-4\n",
    "MODEL = 'VAE-NF'  # VAE-NF | VAE\n",
    "\n",
    "N_FLOWS = 2\n",
    "Z_DIM = 3\n",
    "\n",
    "n_steps = 0\n",
    "model = VAE_NF(N_FLOWS, Z_DIM).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/data/t3home000/spark/LHCOlympics_previous/LHC-Olympics/Code/models/VAE_nf_v1.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mass_and_loss_bkg(inputstring):\n",
    "    f_in = pd.read_hdf(inputstring)\n",
    "    dt_in = f_in.values\n",
    "    dt_in[:,1] = (dt_in[:,1]-np.mean(dt_in[:,1]))/np.std(dt_in[:,1])\n",
    "    dt_in[:,2] = (dt_in[:,2]-np.mean(dt_in[:,2]))/np.std(dt_in[:,2])\n",
    "    dt_in[:,3] = (dt_in[:,3]-np.mean(dt_in[:,3]))/np.std(dt_in[:,3])\n",
    "    dt_in[:,4] = (dt_in[:,4]-np.mean(dt_in[:,4]))/np.std(dt_in[:,4])\n",
    "    dt_in[:,5] = (dt_in[:,5]-np.mean(dt_in[:,5]))/np.std(dt_in[:,5])\n",
    "    dt_in[:,6] = (dt_in[:,6]-np.mean(dt_in[:,6]))/np.std(dt_in[:,6])\n",
    "\n",
    "    dt_in[:,8] = (dt_in[:,8]-np.mean(dt_in[:,8]))/np.std(dt_in[:,8])\n",
    "    dt_in[:,9] = (dt_in[:,9]-np.mean(dt_in[:,9]))/np.std(dt_in[:,9])\n",
    "    dt_in[:,10] = (dt_in[:,10]-np.mean(dt_in[:,10]))/np.std(dt_in[:,10])\n",
    "    dt_in[:,11] = (dt_in[:,11]-np.mean(dt_in[:,11]))/np.std(dt_in[:,11])\n",
    "    dt_in[:,12] = (dt_in[:,12]-np.mean(dt_in[:,12]))/np.std(dt_in[:,12])\n",
    "\n",
    "    dt_in[:,14] = (dt_in[:,14]-np.mean(dt_in[:,14]))/np.std(dt_in[:,14])\n",
    "    dt_in[:,15] = (dt_in[:,15]-np.mean(dt_in[:,15]))/np.std(dt_in[:,15])\n",
    "    dt_in[:,16] = (dt_in[:,16]-np.mean(dt_in[:,16]))/np.std(dt_in[:,16])\n",
    "    dt_in[:,17] = (dt_in[:,17]-np.mean(dt_in[:,17]))/np.std(dt_in[:,17])\n",
    "    dt_in[:,18] = (dt_in[:,18]-np.mean(dt_in[:,18]))/np.std(dt_in[:,18])\n",
    "    dt_in[:,19] = (dt_in[:,19]-np.mean(dt_in[:,19]))/np.std(dt_in[:,19])\n",
    "    \n",
    "    dt_in[:,21] = (dt_in[:,21]-np.mean(dt_in[:,21]))/np.std(dt_in[:,21])\n",
    "    dt_in[:,22] = (dt_in[:,22]-np.mean(dt_in[:,22]))/np.std(dt_in[:,22])\n",
    "    dt_in[:,23] = (dt_in[:,23]-np.mean(dt_in[:,23]))/np.std(dt_in[:,23])\n",
    "    dt_in[:,24] = (dt_in[:,24]-np.mean(dt_in[:,24]))/np.std(dt_in[:,24])\n",
    "    dt_in[:,25] = (dt_in[:,25]-np.mean(dt_in[:,25]))/np.std(dt_in[:,25])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    total_in = torch.tensor(dt_in)\n",
    "    total_in_train_x_1 = total_in.t()[1:7].t()\n",
    "    total_in_train_x_2 = total_in.t()[8:13].t()\n",
    "    total_in_train_x_3 = total_in.t()[14:20].t()\n",
    "    total_in_train_x_4 = total_in.t()[21:26].t()\n",
    "    total_in_selection = torch.cat((total_in_train_x_1,total_in_train_x_2,total_in_train_x_3,total_in_train_x_4),dim=1)\n",
    "    \n",
    "    loss_total_in = torch.mean((model(total_in_selection.float().cuda())[0]-\n",
    "                       total_in_selection.float().cuda())**2,dim=1).data.cpu().numpy()\n",
    "    \n",
    "    f_in = pd.read_hdf(inputstring)\n",
    "    dt_in = f_in.values\n",
    "    \n",
    "    return dt_in[:,0], dt_in[:,10], dt_in[:,23], dt_in[:,9], dt_in[:,22], loss_total_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mass_and_loss_bb(inputstring,bb_idx):\n",
    "    f_in = pd.read_hdf(inputstring)\n",
    "    dt_in = f_in.values\n",
    "    print('shape',bb_idx.shape)\n",
    "    print('before: ', dt_in.shape)\n",
    "    dt_in = dt_in[bb_idx,:]\n",
    "    print('after: ',dt_in.shape)\n",
    "    dt_in[:,1] = (dt_in[:,1]-np.mean(dt_in[:,1]))/np.std(dt_in[:,1])\n",
    "    dt_in[:,2] = (dt_in[:,2]-np.mean(dt_in[:,2]))/np.std(dt_in[:,2])\n",
    "    dt_in[:,3] = (dt_in[:,3]-np.mean(dt_in[:,3]))/np.std(dt_in[:,3])\n",
    "    dt_in[:,4] = (dt_in[:,4]-np.mean(dt_in[:,4]))/np.std(dt_in[:,4])\n",
    "    dt_in[:,5] = (dt_in[:,5]-np.mean(dt_in[:,5]))/np.std(dt_in[:,5])\n",
    "    dt_in[:,6] = (dt_in[:,6]-np.mean(dt_in[:,6]))/np.std(dt_in[:,6])\n",
    "\n",
    "    dt_in[:,8] = (dt_in[:,8]-np.mean(dt_in[:,8]))/np.std(dt_in[:,8])\n",
    "    dt_in[:,9] = (dt_in[:,9]-np.mean(dt_in[:,9]))/np.std(dt_in[:,9])\n",
    "    dt_in[:,10] = (dt_in[:,10]-np.mean(dt_in[:,10]))/np.std(dt_in[:,10])\n",
    "    dt_in[:,11] = (dt_in[:,11]-np.mean(dt_in[:,11]))/np.std(dt_in[:,11])\n",
    "    dt_in[:,12] = (dt_in[:,12]-np.mean(dt_in[:,12]))/np.std(dt_in[:,12])\n",
    "\n",
    "    dt_in[:,14] = (dt_in[:,14]-np.mean(dt_in[:,14]))/np.std(dt_in[:,14])\n",
    "    dt_in[:,15] = (dt_in[:,15]-np.mean(dt_in[:,15]))/np.std(dt_in[:,15])\n",
    "    dt_in[:,16] = (dt_in[:,16]-np.mean(dt_in[:,16]))/np.std(dt_in[:,16])\n",
    "    dt_in[:,17] = (dt_in[:,17]-np.mean(dt_in[:,17]))/np.std(dt_in[:,17])\n",
    "    dt_in[:,18] = (dt_in[:,18]-np.mean(dt_in[:,18]))/np.std(dt_in[:,18])\n",
    "    dt_in[:,19] = (dt_in[:,19]-np.mean(dt_in[:,19]))/np.std(dt_in[:,19])\n",
    "    \n",
    "    dt_in[:,21] = (dt_in[:,21]-np.mean(dt_in[:,21]))/np.std(dt_in[:,21])\n",
    "    dt_in[:,22] = (dt_in[:,22]-np.mean(dt_in[:,22]))/np.std(dt_in[:,22])\n",
    "    dt_in[:,23] = (dt_in[:,23]-np.mean(dt_in[:,23]))/np.std(dt_in[:,23])\n",
    "    dt_in[:,24] = (dt_in[:,24]-np.mean(dt_in[:,24]))/np.std(dt_in[:,24])\n",
    "    dt_in[:,25] = (dt_in[:,25]-np.mean(dt_in[:,25]))/np.std(dt_in[:,25])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    total_in = torch.tensor(dt_in)\n",
    "    total_in_train_x_1 = total_in.t()[1:7].t()\n",
    "    total_in_train_x_2 = total_in.t()[8:13].t()\n",
    "    total_in_train_x_3 = total_in.t()[14:20].t()\n",
    "    total_in_train_x_4 = total_in.t()[21:26].t()\n",
    "    total_in_selection = torch.cat((total_in_train_x_1,total_in_train_x_2,total_in_train_x_3,total_in_train_x_4),dim=1)\n",
    "    \n",
    "    loss_total_in = torch.mean((model(total_in_selection.float().cuda())[0]-\n",
    "                       total_in_selection.float().cuda())**2,dim=1).data.cpu().numpy()\n",
    "    \n",
    "    return dt_in[:,0], loss_total_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blackboxmaker(dt, sig_idx, bkg_idx, num_sig):\n",
    "    train, test = train_test_split(sig_idx,test_size=num_sig)\n",
    "    print(train.shape)\n",
    "    blackboxidx = np.append(bkg_idx,test)\n",
    "    train_with_idx(dt, train, blackboxidx, num_sig)\n",
    "    bbmass, bbloss = get_mass_and_loss_bb(\"/data/t3home000/spark/LHCOlympics/data/MassRatio_RandD.h5\",blackboxidx)\n",
    "    purebkgmass, purebkgmmdt1, purebkgmmdt2, purebkgprun1,purebkgprun2, purebkgloss = get_mass_and_loss_bkg(\"/data/t3home000/spark/LHCOlympics/data/MassRatio_pureBkg.h5\")\n",
    "    np.save(f'mass_numsig_{num_sig}',bbmass)\n",
    "    np.save('mass_bkg.npy',purebkgmass)\n",
    "    np.save(f'bkgaeloss_numsig_{num_sig}',bbloss)\n",
    "    np.save('bkgaeloss_bkg.npy',purebkgloss)\n",
    "    #np.save(f'blackboxidx_{num_sig}', blackboxidx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000,)\n",
      "Epoch: 0 Loss Training: 0.7304543\n",
      "patience counter:  0\n",
      "Epoch: 1 Loss Training: 0.69541687\n",
      "patience counter:  0\n",
      "Epoch: 2 Loss Training: 0.6275796\n",
      "patience counter:  0\n",
      "Epoch: 3 Loss Training: 0.6517631\n",
      "patience counter:  1\n",
      "Epoch: 4 Loss Training: 0.6588153\n",
      "patience counter:  2\n",
      "Epoch: 5 Loss Training: 0.5683471\n",
      "patience counter:  0\n",
      "Epoch: 6 Loss Training: 0.58594525\n",
      "patience counter:  1\n",
      "Epoch: 7 Loss Training: 0.6989092\n",
      "patience counter:  2\n",
      "Epoch: 8 Loss Training: 0.5892665\n",
      "patience counter:  3\n",
      "Epoch: 9 Loss Training: 0.6124245\n",
      "patience counter:  4\n",
      "Epoch: 10 Loss Training: 0.5042583\n",
      "patience counter:  0\n",
      "Epoch: 11 Loss Training: 0.54556894\n",
      "patience counter:  1\n",
      "Epoch: 12 Loss Training: 0.5205042\n",
      "patience counter:  2\n",
      "Epoch: 13 Loss Training: 0.5738252\n",
      "patience counter:  3\n",
      "Epoch: 14 Loss Training: 0.4737109\n",
      "patience counter:  0\n",
      "Epoch: 15 Loss Training: 0.46958837\n",
      "patience counter:  0\n",
      "Epoch: 16 Loss Training: 0.5386655\n",
      "patience counter:  1\n",
      "Epoch: 17 Loss Training: 0.44508067\n",
      "patience counter:  0\n",
      "Epoch: 18 Loss Training: 0.444392\n",
      "patience counter:  0\n",
      "Epoch: 19 Loss Training: 0.52811354\n",
      "patience counter:  1\n",
      "Epoch: 20 Loss Training: 0.44229233\n",
      "patience counter:  0\n",
      "Epoch: 21 Loss Training: 0.43210897\n",
      "patience counter:  0\n",
      "Epoch: 22 Loss Training: 0.51255524\n",
      "patience counter:  1\n",
      "Epoch: 23 Loss Training: 0.42855987\n",
      "patience counter:  0\n",
      "Epoch: 24 Loss Training: 0.47247055\n",
      "patience counter:  1\n",
      "Epoch: 25 Loss Training: 0.49459586\n",
      "patience counter:  2\n",
      "Epoch: 26 Loss Training: 0.44909552\n",
      "patience counter:  3\n",
      "Epoch: 27 Loss Training: 0.49023706\n",
      "patience counter:  4\n",
      "Epoch: 28 Loss Training: 0.4450502\n",
      "patience counter:  5\n",
      "Epoch: 29 Loss Training: 0.43706223\n",
      "patience counter:  6\n",
      "Epoch: 30 Loss Training: 0.40917078\n",
      "patience counter:  0\n",
      "Epoch: 31 Loss Training: 0.4123206\n",
      "patience counter:  1\n",
      "Epoch: 32 Loss Training: 0.43358237\n",
      "patience counter:  2\n",
      "Epoch: 33 Loss Training: 0.4451142\n",
      "patience counter:  3\n",
      "Epoch: 34 Loss Training: 0.4377853\n",
      "patience counter:  4\n",
      "Epoch: 35 Loss Training: 0.42256987\n",
      "patience counter:  5\n",
      "Epoch: 36 Loss Training: 0.45311782\n",
      "patience counter:  6\n",
      "Epoch: 37 Loss Training: 0.45209396\n",
      "patience counter:  7\n",
      "Epoch: 38 Loss Training: 0.44890565\n",
      "patience counter:  8\n",
      "Epoch: 39 Loss Training: 0.37231976\n",
      "patience counter:  0\n",
      "Epoch: 40 Loss Training: 0.49237764\n",
      "patience counter:  1\n",
      "Epoch: 41 Loss Training: 0.39100152\n",
      "patience counter:  2\n",
      "Epoch: 42 Loss Training: 0.37752357\n",
      "patience counter:  3\n",
      "Epoch: 43 Loss Training: 0.41667825\n",
      "patience counter:  4\n",
      "Epoch: 44 Loss Training: 0.51560557\n",
      "patience counter:  5\n",
      "Epoch: 45 Loss Training: 0.4002063\n",
      "patience counter:  6\n",
      "Epoch: 46 Loss Training: 0.3848746\n",
      "patience counter:  7\n",
      "Epoch: 47 Loss Training: 0.37442946\n",
      "patience counter:  8\n",
      "Epoch: 48 Loss Training: 0.3707067\n",
      "patience counter:  0\n",
      "Epoch: 49 Loss Training: 0.53033996\n",
      "patience counter:  1\n",
      "Epoch: 50 Loss Training: 0.46796086\n",
      "patience counter:  2\n",
      "Epoch: 51 Loss Training: 0.43605134\n",
      "patience counter:  3\n",
      "Epoch: 52 Loss Training: 0.45447272\n",
      "patience counter:  4\n",
      "Epoch: 53 Loss Training: 0.4104355\n",
      "patience counter:  5\n",
      "Epoch: 54 Loss Training: 0.46824235\n",
      "patience counter:  6\n",
      "Epoch: 55 Loss Training: 0.36023653\n",
      "patience counter:  0\n",
      "Epoch: 56 Loss Training: 0.36388\n",
      "patience counter:  1\n",
      "Epoch: 57 Loss Training: 0.422689\n",
      "patience counter:  2\n",
      "Epoch: 58 Loss Training: 0.3766508\n",
      "patience counter:  3\n",
      "Epoch: 59 Loss Training: 0.37496614\n",
      "patience counter:  4\n",
      "Epoch: 60 Loss Training: 0.37268323\n",
      "patience counter:  5\n",
      "Epoch: 61 Loss Training: 0.42983988\n",
      "patience counter:  6\n",
      "Epoch: 62 Loss Training: 0.34134465\n",
      "patience counter:  0\n",
      "Epoch: 63 Loss Training: 0.39787206\n",
      "patience counter:  1\n",
      "Epoch: 64 Loss Training: 0.41995636\n",
      "patience counter:  2\n",
      "Epoch: 65 Loss Training: 0.3876507\n",
      "patience counter:  3\n",
      "Epoch: 66 Loss Training: 0.45629323\n",
      "patience counter:  4\n",
      "Epoch: 67 Loss Training: 0.41625315\n",
      "patience counter:  5\n",
      "Epoch: 68 Loss Training: 0.4205063\n",
      "patience counter:  6\n",
      "Epoch: 69 Loss Training: 0.38365886\n",
      "patience counter:  7\n",
      "Epoch: 70 Loss Training: 0.36774647\n",
      "patience counter:  8\n",
      "Epoch: 71 Loss Training: 0.3699065\n",
      "patience counter:  9\n",
      "Epoch: 72 Loss Training: 0.34084362\n",
      "patience counter:  0\n",
      "Epoch: 73 Loss Training: 0.40869036\n",
      "patience counter:  1\n",
      "Epoch: 74 Loss Training: 0.49493876\n",
      "patience counter:  2\n",
      "Epoch: 75 Loss Training: 0.39127272\n",
      "patience counter:  3\n",
      "Epoch: 76 Loss Training: 0.3854661\n",
      "patience counter:  4\n",
      "Epoch: 77 Loss Training: 0.41055822\n",
      "patience counter:  5\n",
      "Epoch: 78 Loss Training: 0.31298673\n",
      "patience counter:  0\n",
      "Epoch: 79 Loss Training: 0.38744444\n",
      "patience counter:  1\n",
      "shape (1010000,)\n",
      "before:  (1100000, 28)\n",
      "after:  (1010000, 28)\n"
     ]
    }
   ],
   "source": [
    "blackboxmaker(dt, signal_idx,bkg_idx, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99000,)\n",
      "Epoch: 0 Loss Training: 0.72924364\n",
      "patience counter:  0\n",
      "Epoch: 1 Loss Training: 0.7680662\n",
      "patience counter:  1\n",
      "Epoch: 2 Loss Training: 0.6995684\n",
      "patience counter:  0\n",
      "Epoch: 3 Loss Training: 0.6701377\n",
      "patience counter:  0\n",
      "Epoch: 4 Loss Training: 0.5060015\n",
      "patience counter:  0\n",
      "Epoch: 5 Loss Training: 0.65874213\n",
      "patience counter:  1\n",
      "Epoch: 6 Loss Training: 0.650142\n",
      "patience counter:  2\n",
      "Epoch: 7 Loss Training: 0.6857457\n",
      "patience counter:  3\n",
      "Epoch: 8 Loss Training: 0.6025545\n",
      "patience counter:  4\n",
      "Epoch: 9 Loss Training: 0.5686222\n",
      "patience counter:  5\n",
      "Epoch: 10 Loss Training: 0.46327937\n",
      "patience counter:  0\n",
      "Epoch: 11 Loss Training: 0.5209136\n",
      "patience counter:  1\n",
      "Epoch: 12 Loss Training: 0.51107574\n",
      "patience counter:  2\n",
      "Epoch: 13 Loss Training: 0.49241415\n",
      "patience counter:  3\n",
      "Epoch: 14 Loss Training: 0.526607\n",
      "patience counter:  4\n",
      "Epoch: 15 Loss Training: 0.42267498\n",
      "patience counter:  0\n",
      "Epoch: 16 Loss Training: 0.50684255\n",
      "patience counter:  1\n",
      "Epoch: 17 Loss Training: 0.40131733\n",
      "patience counter:  0\n",
      "Epoch: 18 Loss Training: 0.5752885\n",
      "patience counter:  1\n",
      "Epoch: 19 Loss Training: 0.43027762\n",
      "patience counter:  2\n",
      "Epoch: 20 Loss Training: 0.3949829\n",
      "patience counter:  0\n",
      "Epoch: 21 Loss Training: 0.51188844\n",
      "patience counter:  1\n",
      "Epoch: 22 Loss Training: 0.37631756\n",
      "patience counter:  0\n",
      "Epoch: 23 Loss Training: 0.33019164\n",
      "patience counter:  0\n",
      "Epoch: 24 Loss Training: 0.34478247\n",
      "patience counter:  1\n",
      "Epoch: 25 Loss Training: 0.42986807\n",
      "patience counter:  2\n",
      "Epoch: 26 Loss Training: 0.3951833\n",
      "patience counter:  3\n",
      "Epoch: 27 Loss Training: 0.29632732\n",
      "patience counter:  0\n",
      "Epoch: 28 Loss Training: 0.35016808\n",
      "patience counter:  1\n",
      "Epoch: 29 Loss Training: 0.40306118\n",
      "patience counter:  2\n",
      "Epoch: 30 Loss Training: 0.34119302\n",
      "patience counter:  3\n",
      "Epoch: 31 Loss Training: 0.29765493\n",
      "patience counter:  4\n",
      "Epoch: 32 Loss Training: 0.30622578\n",
      "patience counter:  5\n",
      "Epoch: 33 Loss Training: 0.40108672\n",
      "patience counter:  6\n",
      "Epoch: 34 Loss Training: 0.3226843\n",
      "patience counter:  7\n",
      "Epoch: 35 Loss Training: 0.32221612\n",
      "patience counter:  8\n",
      "Epoch: 36 Loss Training: 0.29584152\n",
      "patience counter:  0\n",
      "Epoch: 37 Loss Training: 0.30283633\n",
      "patience counter:  1\n",
      "Epoch: 38 Loss Training: 0.36110336\n",
      "patience counter:  2\n",
      "Epoch: 39 Loss Training: 0.37320858\n",
      "patience counter:  3\n",
      "Epoch: 40 Loss Training: 0.30368772\n",
      "patience counter:  4\n",
      "Epoch: 41 Loss Training: 0.3166149\n",
      "patience counter:  5\n",
      "Epoch: 42 Loss Training: 0.27122575\n",
      "patience counter:  0\n",
      "Epoch: 43 Loss Training: 0.29900897\n",
      "patience counter:  1\n",
      "Epoch: 44 Loss Training: 0.32291043\n",
      "patience counter:  2\n",
      "Epoch: 45 Loss Training: 0.30383754\n",
      "patience counter:  3\n",
      "Epoch: 46 Loss Training: 0.31191224\n",
      "patience counter:  4\n",
      "Epoch: 47 Loss Training: 0.4692303\n",
      "patience counter:  5\n",
      "Epoch: 48 Loss Training: 0.28842777\n",
      "patience counter:  6\n",
      "Epoch: 49 Loss Training: 0.32245883\n",
      "patience counter:  7\n",
      "Epoch: 50 Loss Training: 0.24183252\n",
      "patience counter:  0\n",
      "Epoch: 51 Loss Training: 0.34637147\n",
      "patience counter:  1\n",
      "Epoch: 52 Loss Training: 0.29564363\n",
      "patience counter:  2\n",
      "Epoch: 53 Loss Training: 0.31327054\n",
      "patience counter:  3\n",
      "Epoch: 54 Loss Training: 0.26430267\n",
      "patience counter:  4\n",
      "Epoch: 55 Loss Training: 0.31642592\n",
      "patience counter:  5\n",
      "Epoch: 56 Loss Training: 0.26048997\n",
      "patience counter:  6\n",
      "Epoch: 57 Loss Training: 0.29722783\n",
      "patience counter:  7\n",
      "Epoch: 58 Loss Training: 0.2933465\n",
      "patience counter:  8\n",
      "Epoch: 59 Loss Training: 0.3452461\n",
      "patience counter:  9\n",
      "Epoch: 60 Loss Training: 0.3533162\n",
      "patience counter:  10\n",
      "maximum patience reached\n",
      "shape (1001000,)\n",
      "before:  (1100000, 28)\n",
      "after:  (1001000, 28)\n"
     ]
    }
   ],
   "source": [
    "blackboxmaker(dt, signal_idx,bkg_idx, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99900,)\n",
      "Epoch: 0 Loss Training: 0.84763885\n",
      "patience counter:  0\n",
      "Epoch: 1 Loss Training: 0.6701685\n",
      "patience counter:  0\n",
      "Epoch: 2 Loss Training: 0.6765547\n",
      "patience counter:  1\n",
      "Epoch: 3 Loss Training: 0.6278823\n",
      "patience counter:  0\n",
      "Epoch: 4 Loss Training: 0.6131298\n",
      "patience counter:  0\n",
      "Epoch: 5 Loss Training: 0.5800111\n",
      "patience counter:  0\n",
      "Epoch: 6 Loss Training: 0.6385804\n",
      "patience counter:  1\n",
      "Epoch: 7 Loss Training: 0.51943153\n",
      "patience counter:  0\n",
      "Epoch: 8 Loss Training: 0.4642059\n",
      "patience counter:  0\n",
      "Epoch: 9 Loss Training: 0.5073471\n",
      "patience counter:  1\n",
      "Epoch: 10 Loss Training: 0.5965235\n",
      "patience counter:  2\n",
      "Epoch: 11 Loss Training: 0.5109427\n",
      "patience counter:  3\n",
      "Epoch: 12 Loss Training: 0.42334425\n",
      "patience counter:  0\n",
      "Epoch: 13 Loss Training: 0.43196702\n",
      "patience counter:  1\n",
      "Epoch: 14 Loss Training: 0.45328492\n",
      "patience counter:  2\n",
      "Epoch: 15 Loss Training: 0.5000589\n",
      "patience counter:  3\n",
      "Epoch: 16 Loss Training: 0.51796055\n",
      "patience counter:  4\n",
      "Epoch: 17 Loss Training: 0.55099756\n",
      "patience counter:  5\n",
      "Epoch: 18 Loss Training: 0.44099218\n",
      "patience counter:  6\n",
      "Epoch: 19 Loss Training: 0.4459759\n",
      "patience counter:  7\n",
      "Epoch: 20 Loss Training: 0.3689583\n",
      "patience counter:  0\n",
      "Epoch: 21 Loss Training: 0.39914122\n",
      "patience counter:  1\n",
      "Epoch: 22 Loss Training: 0.49001598\n",
      "patience counter:  2\n",
      "Epoch: 23 Loss Training: 0.4195196\n",
      "patience counter:  3\n",
      "Epoch: 24 Loss Training: 0.49098852\n",
      "patience counter:  4\n",
      "Epoch: 25 Loss Training: 0.4111408\n",
      "patience counter:  5\n",
      "Epoch: 26 Loss Training: 0.39459264\n",
      "patience counter:  6\n",
      "Epoch: 27 Loss Training: 0.40138227\n",
      "patience counter:  7\n",
      "Epoch: 28 Loss Training: 0.41564533\n",
      "patience counter:  8\n",
      "Epoch: 29 Loss Training: 0.45227\n",
      "patience counter:  9\n",
      "Epoch: 30 Loss Training: 0.527488\n",
      "patience counter:  10\n",
      "maximum patience reached\n",
      "shape (1000100,)\n",
      "before:  (1100000, 28)\n",
      "after:  (1000100, 28)\n"
     ]
    }
   ],
   "source": [
    "blackboxmaker(dt, signal_idx,bkg_idx, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99990,)\n",
      "Epoch: 0 Loss Training: 0.7569997\n",
      "patience counter:  0\n",
      "Epoch: 1 Loss Training: 0.7088797\n",
      "patience counter:  0\n",
      "Epoch: 2 Loss Training: 0.67568237\n",
      "patience counter:  0\n",
      "Epoch: 3 Loss Training: 0.68582803\n",
      "patience counter:  1\n",
      "Epoch: 4 Loss Training: 0.5446771\n",
      "patience counter:  0\n",
      "Epoch: 5 Loss Training: 0.6457723\n",
      "patience counter:  1\n",
      "Epoch: 6 Loss Training: 0.5585306\n",
      "patience counter:  2\n",
      "Epoch: 7 Loss Training: 0.6205943\n",
      "patience counter:  3\n",
      "Epoch: 8 Loss Training: 0.42225286\n",
      "patience counter:  0\n",
      "Epoch: 9 Loss Training: 0.526603\n",
      "patience counter:  1\n",
      "Epoch: 10 Loss Training: 0.41971257\n",
      "patience counter:  0\n",
      "Epoch: 11 Loss Training: 0.47061148\n",
      "patience counter:  1\n",
      "Epoch: 12 Loss Training: 0.3851867\n",
      "patience counter:  0\n",
      "Epoch: 13 Loss Training: 0.5195433\n",
      "patience counter:  1\n",
      "Epoch: 14 Loss Training: 0.50439763\n",
      "patience counter:  2\n",
      "Epoch: 15 Loss Training: 0.4914897\n",
      "patience counter:  3\n",
      "Epoch: 16 Loss Training: 0.43288893\n",
      "patience counter:  4\n",
      "Epoch: 17 Loss Training: 0.46080163\n",
      "patience counter:  5\n",
      "Epoch: 18 Loss Training: 0.4615974\n",
      "patience counter:  6\n",
      "Epoch: 19 Loss Training: 0.40729848\n",
      "patience counter:  7\n",
      "Epoch: 20 Loss Training: 0.45966992\n",
      "patience counter:  8\n",
      "Epoch: 21 Loss Training: 0.4485207\n",
      "patience counter:  9\n",
      "Epoch: 22 Loss Training: 0.42954928\n",
      "patience counter:  10\n",
      "maximum patience reached\n",
      "shape (1000010,)\n",
      "before:  (1100000, 28)\n",
      "after:  (1000010, 28)\n"
     ]
    }
   ],
   "source": [
    "blackboxmaker(dt, signal_idx,bkg_idx, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

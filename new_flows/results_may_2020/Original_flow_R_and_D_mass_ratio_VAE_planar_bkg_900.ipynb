{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import itertools\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.utils.data as utils\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from argparse import ArgumentParser\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "from flows import RealNVP\n",
    "from models import NormalizingFlowModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_PureBkg = pd.read_hdf(\"../../../2_lhc/LHC_Olympics2020/processing/test_dataset/MassRatio_pureBkg.h5\")\n",
    "dt_PureBkg = f_PureBkg.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_PureBkg[:,1] = (dt_PureBkg[:,1]-np.mean(dt_PureBkg[:,1]))/np.std(dt_PureBkg[:,1])\n",
    "dt_PureBkg[:,2] = (dt_PureBkg[:,2]-np.mean(dt_PureBkg[:,2]))/np.std(dt_PureBkg[:,2])\n",
    "dt_PureBkg[:,3] = (dt_PureBkg[:,3]-np.mean(dt_PureBkg[:,3]))/np.std(dt_PureBkg[:,3])\n",
    "dt_PureBkg[:,4] = (dt_PureBkg[:,4]-np.mean(dt_PureBkg[:,4]))/np.std(dt_PureBkg[:,4])\n",
    "dt_PureBkg[:,5] = (dt_PureBkg[:,5]-np.mean(dt_PureBkg[:,5]))/np.std(dt_PureBkg[:,5])\n",
    "dt_PureBkg[:,6] = (dt_PureBkg[:,6]-np.mean(dt_PureBkg[:,6]))/np.std(dt_PureBkg[:,6])\n",
    "\n",
    "\n",
    "dt_PureBkg[:,8] = (dt_PureBkg[:,8]-np.mean(dt_PureBkg[:,8]))/np.std(dt_PureBkg[:,8])\n",
    "dt_PureBkg[:,9] = (dt_PureBkg[:,9]-np.mean(dt_PureBkg[:,9]))/np.std(dt_PureBkg[:,9])\n",
    "dt_PureBkg[:,10] = (dt_PureBkg[:,10]-np.mean(dt_PureBkg[:,10]))/np.std(dt_PureBkg[:,10])\n",
    "dt_PureBkg[:,11] = (dt_PureBkg[:,11]-np.mean(dt_PureBkg[:,11]))/np.std(dt_PureBkg[:,11])\n",
    "dt_PureBkg[:,12] = (dt_PureBkg[:,12]-np.mean(dt_PureBkg[:,12]))/np.std(dt_PureBkg[:,12])\n",
    "\n",
    "dt_PureBkg[:,14] = (dt_PureBkg[:,14]-np.mean(dt_PureBkg[:,14]))/np.std(dt_PureBkg[:,14])\n",
    "dt_PureBkg[:,15] = (dt_PureBkg[:,15]-np.mean(dt_PureBkg[:,15]))/np.std(dt_PureBkg[:,15])\n",
    "dt_PureBkg[:,16] = (dt_PureBkg[:,16]-np.mean(dt_PureBkg[:,16]))/np.std(dt_PureBkg[:,16])\n",
    "dt_PureBkg[:,17] = (dt_PureBkg[:,17]-np.mean(dt_PureBkg[:,17]))/np.std(dt_PureBkg[:,17])\n",
    "dt_PureBkg[:,18] = (dt_PureBkg[:,18]-np.mean(dt_PureBkg[:,18]))/np.std(dt_PureBkg[:,18])\n",
    "dt_PureBkg[:,19] = (dt_PureBkg[:,19]-np.mean(dt_PureBkg[:,19]))/np.std(dt_PureBkg[:,19])\n",
    "\n",
    "\n",
    "dt_PureBkg[:,21] = (dt_PureBkg[:,21]-np.mean(dt_PureBkg[:,21]))/np.std(dt_PureBkg[:,21])\n",
    "dt_PureBkg[:,22] = (dt_PureBkg[:,22]-np.mean(dt_PureBkg[:,22]))/np.std(dt_PureBkg[:,22])\n",
    "dt_PureBkg[:,23] = (dt_PureBkg[:,23]-np.mean(dt_PureBkg[:,23]))/np.std(dt_PureBkg[:,23])\n",
    "dt_PureBkg[:,24] = (dt_PureBkg[:,24]-np.mean(dt_PureBkg[:,24]))/np.std(dt_PureBkg[:,24])\n",
    "dt_PureBkg[:,25] = (dt_PureBkg[:,25]-np.mean(dt_PureBkg[:,25]))/np.std(dt_PureBkg[:,25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_PureBkg = torch.tensor(dt_PureBkg)\n",
    "total_PureBkg_train_x_1 = total_PureBkg.t()[1:7].t()\n",
    "total_PureBkg_train_x_2 = total_PureBkg.t()[8:13].t()\n",
    "total_PureBkg_train_x_3 = total_PureBkg.t()[14:20].t()\n",
    "total_PureBkg_train_x_4 = total_PureBkg.t()[21:26].t()\n",
    "\n",
    "total_PureBkg_selection = torch.cat((total_PureBkg_train_x_1,total_PureBkg_train_x_2,total_PureBkg_train_x_3,total_PureBkg_train_x_4),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1000\n",
    "#bkgAE_dataset = utils.TensorDataset(data_train_x,data_train_x) \n",
    "bkgAE_train_iterator = utils.DataLoader(total_PureBkg_selection, batch_size=bs, shuffle=True) \n",
    "bkgAE_test_iterator = utils.DataLoader(total_PureBkg_selection, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanarFlow(nn.Module):\n",
    "    def __init__(self, D):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "\n",
    "    def forward(self, z, lamda):\n",
    "        '''\n",
    "        z - latents from prev layer\n",
    "        lambda - Flow parameters (b, w, u)\n",
    "        b - scalar\n",
    "        w - vector\n",
    "        u - vector\n",
    "        '''\n",
    "        b = lamda[:, :1]\n",
    "        w, u = lamda[:, 1:].chunk(2, dim=1)\n",
    "\n",
    "        # Forward\n",
    "        # f(z) = z + u tanh(w^T z + b)\n",
    "        transf = F.tanh(\n",
    "            z.unsqueeze(1).bmm(w.unsqueeze(2))[:, 0] + b\n",
    "        )\n",
    "        f_z = z + u * transf\n",
    "\n",
    "        # Inverse\n",
    "        # psi_z = tanh' (w^T z + b) w\n",
    "        psi_z = (1 - transf ** 2) * w\n",
    "        log_abs_det_jacobian = torch.log(\n",
    "            (1 + psi_z.unsqueeze(1).bmm(u.unsqueeze(2))).abs()\n",
    "        )\n",
    "\n",
    "        return f_z, log_abs_det_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizingFlow(nn.Module):\n",
    "    def __init__(self, K, D):\n",
    "        super().__init__()\n",
    "        self.flows = nn.ModuleList([PlanarFlow(D) for i in range(K)])\n",
    "\n",
    "    def forward(self, z_k, flow_params):\n",
    "        # ladj -> log abs det jacobian\n",
    "        sum_ladj = 0\n",
    "        for i, flow in enumerate(self.flows):\n",
    "            z_k, ladj_k = flow(z_k, flow_params[i])\n",
    "            sum_ladj += ladj_k\n",
    "\n",
    "        return z_k, sum_ladj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_NF(nn.Module):\n",
    "    def __init__(self, K, D):\n",
    "        super().__init__()\n",
    "        self.dim = D\n",
    "        self.K = K\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(22, 96),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(96, 48),\n",
    "            nn.LeakyReLU(True),            \n",
    "            nn.Linear(48, D * 2 + K * (D * 2 + 1))\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(D, 48),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(48, 96),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Linear(96, 22),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.flows = NormalizingFlow(K, D)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Run Encoder and get NF params\n",
    "        enc = self.encoder(x)\n",
    "        mu = enc[:, :self.dim]\n",
    "        log_var = enc[:, self.dim: self.dim * 2]\n",
    "        flow_params = enc[:, 2 * self.dim:].chunk(self.K, dim=1)\n",
    "\n",
    "        # Re-parametrize\n",
    "        sigma = (log_var * .5).exp()\n",
    "        z = mu + sigma * torch.randn_like(sigma)\n",
    "        kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "        # Construct more expressive posterior with NF\n",
    "        z_k, sum_ladj = self.flows(z, flow_params)\n",
    "        kl_div = kl_div / x.size(0) - sum_ladj.mean()  # mean over batch\n",
    "\n",
    "        # Run Decoder\n",
    "        x_prime = self.decoder(z_k)\n",
    "        return x_prime, kl_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating InstanceÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "N_EPOCHS = 80\n",
    "PRINT_INTERVAL = 2000\n",
    "NUM_WORKERS = 4\n",
    "LR = 1e-4\n",
    "MODEL = 'VAE-NF'  # VAE-NF | VAE\n",
    "\n",
    "N_FLOWS = 2\n",
    "Z_DIM = 3\n",
    "\n",
    "n_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL == 'VAE-NF':\n",
    "    model = VAE_NF(N_FLOWS, Z_DIM).cuda()\n",
    "else:\n",
    "    model = VAE(Z_DIM).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    global n_steps\n",
    "    train_loss = []\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, x in enumerate(bkgAE_train_iterator):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        x = x.float().cuda()\n",
    "\n",
    "        x_tilde, kl_div = model(x)\n",
    "        loss_recons = F.binary_cross_entropy(x_tilde, x, size_average=False) / x.size(0)\n",
    "        loss = loss_recons + kl_div\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss.append([loss_recons.item(), kl_div.item()])\n",
    "        #writer.add_scalar('loss/train/ELBO', loss.item(), n_steps)\n",
    "        #writer.add_scalar('loss/train/reconstruction', loss_recons.item(), n_steps)\n",
    "        #writer.add_scalar('loss/train/KL', kl_div.item(), n_steps)\n",
    "\n",
    "        if (batch_idx + 1) % PRINT_INTERVAL == 0:\n",
    "            print('\\tIter [{}/{} ({:.0f}%)]\\tLoss: {} Time: {:5.3f} ms/batch'.format(\n",
    "                batch_idx * len(x), 50000,\n",
    "                PRINT_INTERVAL * batch_idx / 50000,\n",
    "                np.asarray(train_loss)[-PRINT_INTERVAL:].mean(0),\n",
    "                1000 * (time.time() - start_time)\n",
    "            ))\n",
    "\n",
    "        n_steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(split='valid'):\n",
    "    global n_steps\n",
    "    start_time = time.time()\n",
    "    val_loss = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, x in enumerate(bkgAE_test_iterator):\n",
    "            \n",
    "            x = x.float().cuda()\n",
    "\n",
    "            x_tilde, kl_div = model(x)\n",
    "            loss_recons = F.binary_cross_entropy(x_tilde, x, size_average=False) / x.size(0)\n",
    "            loss = loss_recons + kl_div\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "            #writer.add_scalar('loss/{}/ELBO'.format(split), loss.item(), n_steps)\n",
    "            #writer.add_scalar('loss/{}/reconstruction'.format(split), loss_recons.item(), n_steps)\n",
    "            #writer.add_scalar('loss/{}/KL'.format(split), kl_div.item(), n_steps)\n",
    "\n",
    "    print('\\nEvaluation Completed ({})!\\tLoss: {:5.4f} Time: {:5.3f} s'.format(\n",
    "        split,\n",
    "        np.asarray(val_loss).mean(0),\n",
    "        time.time() - start_time\n",
    "    ))\n",
    "    return np.asarray(val_loss).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_LOSS = 99999\n",
    "LAST_SAVED = -1\n",
    "for epoch in range(1, N_EPOCHS):\n",
    "    print(\"Epoch {}:\".format(epoch))\n",
    "    train()\n",
    "    cur_loss = evaluate()\n",
    "\n",
    "    if cur_loss <= BEST_LOSS:\n",
    "        BEST_LOSS = cur_loss\n",
    "        LAST_SAVED = epoch\n",
    "        print(\"Saving model!\")\n",
    "        torch.save(model.state_dict(), \"lhc_weights/original_bkg_vae_NF_planar.h5\")\n",
    "    else:\n",
    "        print(\"Not saving model! Last saved: {}\".format(LAST_SAVED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mass_and_loss(inputstring):\n",
    "    f_in = pd.read_hdf(inputstring)\n",
    "    dt_in = f_in.values\n",
    "    dt_in[:,1] = (dt_in[:,1]-np.mean(dt_in[:,1]))/np.std(dt_in[:,1])\n",
    "    dt_in[:,2] = (dt_in[:,2]-np.mean(dt_in[:,2]))/np.std(dt_in[:,2])\n",
    "    dt_in[:,3] = (dt_in[:,3]-np.mean(dt_in[:,3]))/np.std(dt_in[:,3])\n",
    "    dt_in[:,4] = (dt_in[:,4]-np.mean(dt_in[:,4]))/np.std(dt_in[:,4])\n",
    "    dt_in[:,5] = (dt_in[:,5]-np.mean(dt_in[:,5]))/np.std(dt_in[:,5])\n",
    "    dt_in[:,6] = (dt_in[:,6]-np.mean(dt_in[:,6]))/np.std(dt_in[:,6])\n",
    "\n",
    "    dt_in[:,8] = (dt_in[:,8]-np.mean(dt_in[:,8]))/np.std(dt_in[:,8])\n",
    "    dt_in[:,9] = (dt_in[:,9]-np.mean(dt_in[:,9]))/np.std(dt_in[:,9])\n",
    "    dt_in[:,10] = (dt_in[:,10]-np.mean(dt_in[:,10]))/np.std(dt_in[:,10])\n",
    "    dt_in[:,11] = (dt_in[:,11]-np.mean(dt_in[:,11]))/np.std(dt_in[:,11])\n",
    "    dt_in[:,12] = (dt_in[:,12]-np.mean(dt_in[:,12]))/np.std(dt_in[:,12])\n",
    "\n",
    "    dt_in[:,14] = (dt_in[:,14]-np.mean(dt_in[:,14]))/np.std(dt_in[:,14])\n",
    "    dt_in[:,15] = (dt_in[:,15]-np.mean(dt_in[:,15]))/np.std(dt_in[:,15])\n",
    "    dt_in[:,16] = (dt_in[:,16]-np.mean(dt_in[:,16]))/np.std(dt_in[:,16])\n",
    "    dt_in[:,17] = (dt_in[:,17]-np.mean(dt_in[:,17]))/np.std(dt_in[:,17])\n",
    "    dt_in[:,18] = (dt_in[:,18]-np.mean(dt_in[:,18]))/np.std(dt_in[:,18])\n",
    "    dt_in[:,19] = (dt_in[:,19]-np.mean(dt_in[:,19]))/np.std(dt_in[:,19])\n",
    "    \n",
    "    dt_in[:,21] = (dt_in[:,21]-np.mean(dt_in[:,21]))/np.std(dt_in[:,21])\n",
    "    dt_in[:,22] = (dt_in[:,22]-np.mean(dt_in[:,22]))/np.std(dt_in[:,22])\n",
    "    dt_in[:,23] = (dt_in[:,23]-np.mean(dt_in[:,23]))/np.std(dt_in[:,23])\n",
    "    dt_in[:,24] = (dt_in[:,24]-np.mean(dt_in[:,24]))/np.std(dt_in[:,24])\n",
    "    dt_in[:,25] = (dt_in[:,25]-np.mean(dt_in[:,25]))/np.std(dt_in[:,25])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    total_in = torch.tensor(dt_in)\n",
    "    total_in_train_x_1 = total_in.t()[1:7].t()\n",
    "    total_in_train_x_2 = total_in.t()[8:13].t()\n",
    "    total_in_train_x_3 = total_in.t()[14:20].t()\n",
    "    total_in_train_x_4 = total_in.t()[21:26].t()\n",
    "    total_in_selection = torch.cat((total_in_train_x_1,total_in_train_x_2,total_in_train_x_3,total_in_train_x_4),dim=1)\n",
    "    \n",
    "    loss_total_in = torch.mean((model(total_in_selection.float().cuda())[0]-\n",
    "                       total_in_selection.float().cuda())**2,dim=1).data.cpu().numpy()\n",
    "    \n",
    "    f_in = pd.read_hdf(inputstring)\n",
    "    dt_in = f_in.values\n",
    "    \n",
    "    return dt_in[:,0], dt_in[:,10], dt_in[:,23], dt_in[:,9], dt_in[:,22], loss_total_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mass(inputstring):\n",
    "\n",
    "    f_in = pd.read_hdf(inputstring)\n",
    "    dt_in = f_in.values\n",
    "    \n",
    "    return dt_in[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb2mass = get_mass(\"../../../2_lhc/LHC_Olympics2020/processing/test_dataset/MassRatio_BB1.h5\")\n",
    "purebkgmass = get_mass(\"../../../2_lhc/LHC_Olympics2020/processing/test_dataset/MassRatio_pureBkg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb2mass, bb2mmdt1, bb2mmdt2, bb2prun1,bb2prun2, bb2loss = get_mass_and_loss(\"../../../2_lhc/LHC_Olympics2020/processing/test_dataset/MassRatio_BB1.h5\")\n",
    "purebkgmass, purebkgmmdt1, purebkgmmdt2, purebkgprun1,purebkgprun2, purebkgloss = get_mass_and_loss(\"../../../2_lhc/LHC_Olympics2020/processing/test_dataset/MassRatio_pureBkg.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "bins = np.linspace(0,5,1100)\n",
    "#plt.hist(purebkgloss[np.where((purebkgmass>6000) & (purebkgmass<7000))[0]],bins=bins,label='background',color=\"blue\");\n",
    "#plt.hist(bb2loss[np.where((bb2mass>6000) & (bb2mass<7000))[0]],bins=bins,label='Blackbox2',histtype='step',color=\"red\");\n",
    "#plt.semilogy()\n",
    "plt.hist(bb2loss,bins=bins,alpha=0.3,color='b',label='blackbox1')\n",
    "plt.hist(purebkgloss,bins=bins,alpha=0.3,color='r',label='background')\n",
    "plt.xlabel(r'Autoencoder Loss')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')\n",
    "# plt.savefig(\"/data/t3home000/spark/LHCOlympics/plots/NF_bkgAE_autoencoder_loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
